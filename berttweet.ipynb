{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "RANDOM_SEED = 577\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, label_encoder):\n",
    "    texts = data['text'].tolist()\n",
    "    labels = label_encoder.fit_transform(data['emoji_id'])\n",
    "    return np.array(texts), labels, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "bertweet = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\", num_labels=100)\n",
    "\n",
    "class BERTweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=25):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# train_data = pd.read_csv(\"dataset/train_data.csv\")\n",
    "train_data = pd.read_csv(\"dataset/train_data_augmented.csv\").dropna()\n",
    "validate_data = pd.read_csv(\"dataset/validate_data.csv\").dropna()\n",
    "test_data = pd.read_csv(\"dataset/test_data.csv\").dropna()\n",
    "\n",
    "train_texts, train_labels, label_encoder = preprocess_data(train_data, label_encoder)\n",
    "test_texts, test_labels, _ = preprocess_data(test_data, label_encoder)\n",
    "validate_texts, validate_labels, _ = preprocess_data(validate_data, label_encoder)\n",
    "\n",
    "\n",
    "train_dataset = BERTweetDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = BERTweetDataset(test_texts, test_labels, tokenizer)\n",
    "validate_dataset = BERTweetDataset(validate_texts, validate_labels, tokenizer)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 33079\n",
      "Test dataset size: 3041\n",
      "Validate dataset size: 3041\n",
      "Train DataLoader size: 259\n",
      "Test DataLoader size: 24\n",
      "Validate DataLoader size: 24\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Validate dataset size: {len(test_dataset)}\")\n",
    "print(f\"Train DataLoader size: {len(train_dataloader)}\")\n",
    "print(f\"Test DataLoader size: {len(test_dataloader)}\")\n",
    "print(f\"Validate DataLoader size: {len(validate_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server0/a/wu1522/miniconda3/envs/cuda11.6-python3-pytorch/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      " 10%|█         | 1/10 [00:59<08:56, 59.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 4.2334, Validation Loss: 3.6180, Validation Accuracy: 0.2493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:56<07:44, 58.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 3.5419, Validation Loss: 3.0803, Validation Accuracy: 0.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:53<06:43, 57.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 3.0291, Validation Loss: 2.7540, Validation Accuracy: 0.3591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:51<05:45, 57.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 2.6297, Validation Loss: 2.5438, Validation Accuracy: 0.4008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [04:48<04:47, 57.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 2.2885, Validation Loss: 2.4176, Validation Accuracy: 0.4143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [05:46<03:49, 57.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 1.9962, Validation Loss: 2.3061, Validation Accuracy: 0.4267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [06:43<02:52, 57.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 1.7351, Validation Loss: 2.2605, Validation Accuracy: 0.4382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [07:41<01:54, 57.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 1.5275, Validation Loss: 2.2202, Validation Accuracy: 0.4415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [08:38<00:57, 57.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 1.3407, Validation Loss: 2.2238, Validation Accuracy: 0.4442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [09:36<00:00, 57.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 1.1883, Validation Loss: 2.1767, Validation Accuracy: 0.4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bertweet.to(device)\n",
    "\n",
    "optimizer = AdamW(bertweet.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    bertweet.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for input_ids, attention_mask, labels in train_dataloader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bertweet(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    bertweet.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in validate_dataloader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = bertweet(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct_predictions / len(validate_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_dataloader):.4f}, Validation Loss: {val_loss / len(validate_dataloader):.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4594\n"
     ]
    }
   ],
   "source": [
    "bertweet.eval()\n",
    "correct_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_dataloader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = bertweet(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct_predictions / len(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3162586629243133\n",
      "F1 Score Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.41      0.38        22\n",
      "           1       0.59      0.67      0.63       380\n",
      "           2       0.50      0.11      0.19        35\n",
      "           3       0.36      0.44      0.39        32\n",
      "           4       0.44      0.73      0.55        22\n",
      "           5       0.65      0.72      0.68        43\n",
      "           6       0.22      0.40      0.29         5\n",
      "           7       0.44      0.59      0.50       182\n",
      "           8       0.00      0.00      0.00        37\n",
      "           9       0.35      0.73      0.47        75\n",
      "          10       0.57      0.67      0.61        82\n",
      "          11       0.50      0.20      0.29        10\n",
      "          12       0.39      0.48      0.43        82\n",
      "          13       0.00      0.00      0.00        11\n",
      "          14       0.00      0.00      0.00        12\n",
      "          15       0.69      0.62      0.65       117\n",
      "          16       0.41      0.76      0.54        79\n",
      "          17       0.34      0.67      0.45        46\n",
      "          18       0.00      0.00      0.00        22\n",
      "          19       0.00      0.00      0.00        22\n",
      "          20       0.40      0.33      0.36        12\n",
      "          21       0.64      0.51      0.57        88\n",
      "          22       0.00      0.00      0.00        28\n",
      "          23       0.50      0.50      0.50       247\n",
      "          24       1.00      0.17      0.29         6\n",
      "          25       0.36      0.46      0.40       108\n",
      "          26       0.62      0.74      0.67        42\n",
      "          27       0.00      0.00      0.00        23\n",
      "          28       1.00      0.18      0.31        11\n",
      "          29       0.00      0.00      0.00        30\n",
      "          30       0.10      0.33      0.15         6\n",
      "          31       0.40      0.54      0.46       214\n",
      "          32       0.25      0.22      0.24         9\n",
      "          33       0.51      0.57      0.54        40\n",
      "          34       0.00      0.00      0.00        28\n",
      "          35       0.31      0.47      0.38       138\n",
      "          36       0.20      0.50      0.29         8\n",
      "          37       0.18      0.12      0.15        16\n",
      "          38       0.65      0.71      0.68        28\n",
      "          39       0.50      0.80      0.62         5\n",
      "          40       0.67      0.67      0.67         3\n",
      "          41       1.00      0.05      0.09        21\n",
      "          42       0.00      0.00      0.00        21\n",
      "          43       0.55      0.46      0.50        13\n",
      "          44       0.33      0.07      0.11        15\n",
      "          45       0.58      0.37      0.45        38\n",
      "          46       0.00      0.00      0.00        10\n",
      "          47       0.50      0.06      0.11        16\n",
      "          48       0.86      0.55      0.67        11\n",
      "          49       0.00      0.00      0.00         4\n",
      "          50       0.00      0.00      0.00        20\n",
      "          51       0.38      0.40      0.39        42\n",
      "          52       0.00      0.00      0.00         3\n",
      "          53       0.31      0.06      0.10        66\n",
      "          54       0.00      0.00      0.00         3\n",
      "          55       0.00      0.00      0.00        31\n",
      "          56       0.00      0.00      0.00         7\n",
      "          57       0.33      0.20      0.25         5\n",
      "          58       1.00      0.60      0.75         5\n",
      "          59       0.40      0.40      0.40         5\n",
      "          60       0.89      0.62      0.73        13\n",
      "          61       0.00      0.00      0.00         8\n",
      "          62       0.57      0.57      0.57         7\n",
      "          63       0.25      0.06      0.09        18\n",
      "          64       1.00      0.06      0.11        17\n",
      "          65       0.50      0.04      0.07        25\n",
      "          66       0.14      0.20      0.17         5\n",
      "          67       0.33      1.00      0.50         2\n",
      "          68       1.00      0.25      0.40         4\n",
      "          69       0.70      0.78      0.74         9\n",
      "          70       1.00      0.20      0.33         5\n",
      "          71       0.67      0.50      0.57         4\n",
      "          72       0.57      0.36      0.44        11\n",
      "          73       0.35      0.55      0.43        11\n",
      "          74       0.57      0.67      0.62         6\n",
      "          75       0.25      0.14      0.18         7\n",
      "          76       0.78      0.70      0.74        10\n",
      "          77       0.00      0.00      0.00         7\n",
      "          78       0.40      0.20      0.27        10\n",
      "          79       0.00      0.00      0.00         5\n",
      "          80       0.69      0.82      0.75        11\n",
      "          81       0.00      0.00      0.00         8\n",
      "          82       0.57      0.57      0.57         7\n",
      "          83       0.00      0.00      0.00         1\n",
      "          84       0.62      0.71      0.67         7\n",
      "          85       0.67      1.00      0.80         2\n",
      "          86       0.00      0.00      0.00         2\n",
      "          87       0.33      1.00      0.50         1\n",
      "          88       0.45      0.62      0.53         8\n",
      "          89       0.00      0.00      0.00         4\n",
      "          90       0.67      0.50      0.57         4\n",
      "          91       0.25      0.25      0.25         4\n",
      "          92       0.00      0.00      0.00         9\n",
      "          93       0.60      0.30      0.40        10\n",
      "          94       0.00      0.00      0.00         7\n",
      "          95       0.50      1.00      0.67         5\n",
      "          96       0.00      0.00      0.00         8\n",
      "          97       0.13      0.22      0.17         9\n",
      "          98       0.50      1.00      0.67         3\n",
      "          99       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.46      3041\n",
      "   macro avg       0.37      0.34      0.32      3041\n",
      "weighted avg       0.43      0.46      0.42      3041\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server0/a/wu1522/miniconda3/envs/cuda11.6-python3-pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/server0/a/wu1522/miniconda3/envs/cuda11.6-python3-pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/server0/a/wu1522/miniconda3/envs/cuda11.6-python3-pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "bertweet.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_dataloader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = bertweet(input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Compute and print the classification report\n",
    "f1_score_report = classification_report(true_labels, predicted_labels)\n",
    "print(f1_score(true_labels, predicted_labels, average=\"macro\"))\n",
    "print(\"F1 Score Report:\")\n",
    "print(f1_score_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6590e-04, 5.9816e-04, 7.2852e-04, 2.4061e-04, 2.4354e-03, 5.3358e-03,\n",
      "         2.1377e-03, 1.3610e-03, 1.3374e-03, 3.2126e-04, 2.3399e-02, 7.1908e-04,\n",
      "         1.5424e-03, 2.3722e-03, 1.6425e-03, 1.4648e-03, 2.5034e-03, 6.2619e-04,\n",
      "         6.6121e-04, 1.9675e-03, 1.0300e-03, 1.6258e-03, 5.8667e-04, 2.1243e-03,\n",
      "         2.4942e-03, 1.6727e-03, 1.0062e-02, 1.1106e-03, 7.1480e-04, 6.3631e-04,\n",
      "         8.3600e-04, 1.4328e-04, 2.0256e-03, 8.8725e-04, 3.8393e-04, 2.0320e-04,\n",
      "         6.6939e-04, 6.2245e-04, 6.0409e-03, 1.1874e-03, 8.7890e-04, 4.7415e-03,\n",
      "         2.8010e-03, 3.1406e-03, 3.7967e-04, 2.2139e-03, 3.8184e-04, 1.1477e-01,\n",
      "         5.2864e-04, 1.5080e-02, 4.4223e-04, 4.3471e-04, 1.9818e-03, 7.9152e-04,\n",
      "         9.0659e-04, 1.2345e-03, 1.4133e-03, 9.1259e-04, 3.2011e-04, 6.5046e-01,\n",
      "         4.1065e-03, 1.5103e-03, 9.8508e-03, 4.3015e-04, 9.9339e-04, 1.5640e-03,\n",
      "         7.6500e-04, 8.3128e-03, 7.4763e-03, 8.0556e-04, 9.7631e-04, 2.0100e-03,\n",
      "         5.1194e-04, 2.7488e-04, 2.4584e-03, 5.8569e-03, 1.4020e-03, 1.0065e-02,\n",
      "         4.4910e-04, 1.2237e-03, 7.9833e-04, 1.3593e-03, 6.3008e-04, 2.1879e-03,\n",
      "         1.1985e-03, 6.1434e-04, 1.0174e-03, 3.1959e-03, 1.0053e-03, 6.7838e-04,\n",
      "         9.5215e-04, 8.4544e-04, 1.2349e-03, 1.1430e-03, 6.1195e-04, 2.6010e-02,\n",
      "         1.5453e-03, 1.4444e-03, 1.1856e-03, 6.6616e-04]], device='cuda:0')\n",
      "Predicted label: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224344/2809076793.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(nn.functional.softmax(outputs.logits))\n"
     ]
    }
   ],
   "source": [
    "def test_tweet(tweet, bertweet_model, tokenizer):\n",
    "    # Step 1: Tokenize and prepare the input tweet for the BERTweet model\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\", max_length=128, padding=\"max_length\", truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Step 2: Feed the input to the BERTweet model and obtain the predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = bertweet_model(input_ids, attention_mask=attention_mask)\n",
    "        print(nn.functional.softmax(outputs.logits))\n",
    "        _, predicted_idx = torch.max(outputs.logits, 1)\n",
    "\n",
    "    predicted_idx = predicted_idx.item()\n",
    "    \n",
    "    return predicted_idx\n",
    "\n",
    "tweet = \"Good night, my friend.\"\n",
    "label = test_tweet(tweet, bertweet, tokenizer)\n",
    "print(f\"Predicted label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_map = {}\n",
    "for i, j in zip(train_data[\"emoji\"], train_data[\"emoji_id\"]):\n",
    "  if i not in emoji_map:\n",
    "    emoji_map[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🌙'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_emoji_map = {value: key for key, value in emoji_map.items()}\n",
    "reversed_emoji_map[label][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' 😊': 25,\n",
       " ' 😱': 0,\n",
       " ' 😔': 7,\n",
       " ' 🌞': 26,\n",
       " ' 🙏': 5,\n",
       " ' 😎': 89,\n",
       " ' 💖': 96,\n",
       " ' 🎬': 91,\n",
       " ' 😂': 9,\n",
       " ' 😴': 10,\n",
       " ' 🙁': 55,\n",
       " ' 🤔': 1,\n",
       " ' 🤩': 31,\n",
       " ' 😢': 16,\n",
       " ' 🤞': 21,\n",
       " ' 🏠': 75,\n",
       " ' 🤬': 3,\n",
       " ' 😞': 53,\n",
       " ' 📝': 90,\n",
       " ' 😩': 12,\n",
       " ' 🎮': 82,\n",
       " ' 🤕': 17,\n",
       " ' ☕️': 62,\n",
       " ' 🙄': 70,\n",
       " ' 🤪': 14,\n",
       " ' 🤦': 35,\n",
       " ' 🧹': 87,\n",
       " ' 🤷': 64,\n",
       " ' 🌙': 59,\n",
       " ' 🌤': 67,\n",
       " ' 😷': 4,\n",
       " ' 🛍': 40,\n",
       " ' 🤗': 23,\n",
       " ' ❤️': 33,\n",
       " ' 🍻': 69,\n",
       " ' 🙅': 97,\n",
       " ' 🤓': 63,\n",
       " ' 🤑': 11,\n",
       " ' 😆': 29,\n",
       " ' 🤝': 2,\n",
       " ' 😡': 36,\n",
       " ' 🎉': 15,\n",
       " ' 💕': 54,\n",
       " ' 🤯': 50,\n",
       " ' 💔': 42,\n",
       " ' 🎂': 52,\n",
       " ' 💰': 85,\n",
       " ' 🤣': 34,\n",
       " ' 🤘': 28,\n",
       " ' 🎧': 99,\n",
       " ' 😒': 81,\n",
       " ' 😠': 57,\n",
       " ' 😤': 18,\n",
       " ' 🔥': 65,\n",
       " ' 😉': 61,\n",
       " ' 🌧': 38,\n",
       " ' 😬': 46,\n",
       " ' 📺': 39,\n",
       " ' 🤢': 20,\n",
       " ' 🛫': 95,\n",
       " ' ☀️': 77,\n",
       " ' 🎵': 30,\n",
       " ' 🚗': 60,\n",
       " ' 🥵': 80,\n",
       " ' 💪': 45,\n",
       " ' 🍴': 6,\n",
       " ' 😍': 51,\n",
       " ' 😋': 98,\n",
       " ' 🏡': 32,\n",
       " ' 🍕': 48,\n",
       " ' 😲': 92,\n",
       " ' 🍔': 88,\n",
       " ' 📖': 76,\n",
       " ' 🎶': 37,\n",
       " ' 😳': 56,\n",
       " ' 😰': 79,\n",
       " ' 🎤': 72,\n",
       " ' 😃': 27,\n",
       " ' 💻': 78,\n",
       " ' 🐶': 43,\n",
       " ' 😕': 94,\n",
       " ' 🙌': 41,\n",
       " ' 🍽': 24,\n",
       " ' 📚': 73,\n",
       " ' 😁': 22,\n",
       " ' 🎸': 66,\n",
       " ' 😭': 8,\n",
       " ' 🤤': 84,\n",
       " ' 🗳': 71,\n",
       " ' 🍿': 86,\n",
       " ' 🏃': 74,\n",
       " ' 🌊': 68,\n",
       " ' 🤠': 49,\n",
       " ' 😣': 44,\n",
       " ' 😖': 13,\n",
       " ' 🤒': 19,\n",
       " ' 🍦': 58,\n",
       " ' 🌩': 83,\n",
       " ' 💤': 47,\n",
       " ' 💃': 93}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [7.9977e-04, 2.9141e-04, 1.7481e-03, 8.9901e-04, 2.2168e-03, 5.4152e-03,\n",
    "         3.6625e-03, 9.3339e-04, 1.6010e-03, 8.9317e-04, 3.2083e-03, 1.6273e-03,\n",
    "         1.4378e-03, 1.4740e-03, 1.4684e-03, 5.8376e-03, 2.0788e-03, 1.0593e-03,\n",
    "         8.9349e-04, 2.4485e-03, 1.5230e-03, 1.3898e-03, 4.0302e-03, 9.9571e-03,\n",
    "         2.1867e-03, 9.8646e-03, 6.6561e-01, 9.3499e-03, 3.0011e-03, 2.2290e-03,\n",
    "         3.0850e-03, 4.2704e-03, 7.1119e-03, 6.8402e-03, 1.4497e-03, 3.4174e-04,\n",
    "         1.2950e-03, 4.0112e-03, 9.7552e-03, 1.9705e-03, 3.2153e-03, 5.6694e-03,\n",
    "         2.2442e-03, 3.4494e-03, 1.1937e-03, 1.6548e-03, 7.6679e-04, 3.4509e-03,\n",
    "         3.1187e-03, 2.7716e-03, 5.4364e-04, 5.7858e-03, 3.6586e-03, 7.9227e-04,\n",
    "         6.3173e-03, 9.1841e-04, 1.0079e-03, 1.0732e-03, 2.7833e-03, 1.0879e-02,\n",
    "         5.2406e-03, 2.5733e-03, 3.5899e-03, 1.0424e-03, 8.7068e-04, 5.2327e-03,\n",
    "         3.0119e-03, 1.2747e-02, 8.6398e-03, 2.4391e-03, 1.6232e-03, 1.9866e-03,\n",
    "         3.7783e-03, 1.2592e-03, 3.2638e-03, 4.1626e-03, 2.1148e-03, 2.6737e-02,\n",
    "         2.2242e-03, 1.3329e-03, 3.9976e-03, 1.5409e-03, 2.0270e-03, 3.8865e-03,\n",
    "         2.4626e-03, 1.3738e-03, 3.1394e-03, 2.0015e-03, 3.2569e-03, 3.7861e-03,\n",
    "         2.2908e-03, 4.3359e-03, 9.9548e-04, 3.3245e-03, 1.2052e-03, 4.4659e-03,\n",
    "         3.8085e-03, 1.6605e-03, 2.9919e-03, 3.0202e-03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.66561: 26,\n",
       " 0.026737: 77,\n",
       " 0.012747: 67,\n",
       " 0.010879: 59,\n",
       " 0.0099571: 23,\n",
       " 0.0098646: 25,\n",
       " 0.0097552: 38,\n",
       " 0.0093499: 27,\n",
       " 0.0086398: 68,\n",
       " 0.0071119: 32,\n",
       " 0.0068402: 33,\n",
       " 0.0063173: 54,\n",
       " 0.0058376: 15,\n",
       " 0.0057858: 51,\n",
       " 0.0056694: 41,\n",
       " 0.0054152: 5,\n",
       " 0.0052406: 60,\n",
       " 0.0052327: 65,\n",
       " 0.0044659: 95,\n",
       " 0.0043359: 91,\n",
       " 0.0042704: 31,\n",
       " 0.0041626: 75,\n",
       " 0.0040302: 22,\n",
       " 0.0040112: 37,\n",
       " 0.0039976: 80,\n",
       " 0.0038865: 83,\n",
       " 0.0038085: 96,\n",
       " 0.0037861: 89,\n",
       " 0.0037783: 72,\n",
       " 0.0036625: 6,\n",
       " 0.0036586: 52,\n",
       " 0.0035899: 62,\n",
       " 0.0034509: 47,\n",
       " 0.0034494: 43,\n",
       " 0.0033245: 93,\n",
       " 0.0032638: 74,\n",
       " 0.0032569: 88,\n",
       " 0.0032153: 40,\n",
       " 0.0032083: 10,\n",
       " 0.0031394: 86,\n",
       " 0.0031187: 48,\n",
       " 0.003085: 30,\n",
       " 0.0030202: 99,\n",
       " 0.0030119: 66,\n",
       " 0.0030011: 28,\n",
       " 0.0029919: 98,\n",
       " 0.0027833: 58,\n",
       " 0.0027716: 49,\n",
       " 0.0025733: 61,\n",
       " 0.0024626: 84,\n",
       " 0.0024485: 19,\n",
       " 0.0024391: 69,\n",
       " 0.0022908: 90,\n",
       " 0.0022442: 42,\n",
       " 0.002229: 29,\n",
       " 0.0022242: 78,\n",
       " 0.0022168: 4,\n",
       " 0.0021867: 24,\n",
       " 0.0021148: 76,\n",
       " 0.0020788: 16,\n",
       " 0.002027: 82,\n",
       " 0.0020015: 87,\n",
       " 0.0019866: 71,\n",
       " 0.0019705: 39,\n",
       " 0.0017481: 2,\n",
       " 0.0016605: 97,\n",
       " 0.0016548: 45,\n",
       " 0.0016273: 11,\n",
       " 0.0016232: 70,\n",
       " 0.001601: 8,\n",
       " 0.0015409: 81,\n",
       " 0.001523: 20,\n",
       " 0.001474: 13,\n",
       " 0.0014684: 14,\n",
       " 0.0014497: 34,\n",
       " 0.0014378: 12,\n",
       " 0.0013898: 21,\n",
       " 0.0013738: 85,\n",
       " 0.0013329: 79,\n",
       " 0.001295: 36,\n",
       " 0.0012592: 73,\n",
       " 0.0012052: 94,\n",
       " 0.0011937: 44,\n",
       " 0.0010732: 57,\n",
       " 0.0010593: 17,\n",
       " 0.0010424: 63,\n",
       " 0.0010079: 56,\n",
       " 0.00099548: 92,\n",
       " 0.00093339: 7,\n",
       " 0.00091841: 55,\n",
       " 0.00089901: 3,\n",
       " 0.00089349: 18,\n",
       " 0.00089317: 9,\n",
       " 0.00087068: 64,\n",
       " 0.00079977: 0,\n",
       " 0.00079227: 53,\n",
       " 0.00076679: 46,\n",
       " 0.00054364: 50,\n",
       " 0.00034174: 35,\n",
       " 0.00029141: 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = {a[i]: i for i in range(len(a))}\n",
    "dict(sorted(b.items(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 🌙'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_emoji_map[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11.6-python3-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
